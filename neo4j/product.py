from py2neo import Graph, Node, Relationship
import os
import pandas as pd
import re
import json
from tqdm import tqdm

class Neo4jKnowledgeGraph:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="123456",confidence=0.82):
        self.confidence=confidence
        """åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“è¿æ¥
        
        Args:
            uri (str): Neo4jæ•°æ®åº“URI
            user (str): ç”¨æˆ·å
            password (str): å¯†ç 
        """
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        cur_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_path = os.path.join(cur_dir, "data")
        
        # Neo4jè¿æ¥é…ç½®
        try:
            #ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
            password = os.getenv("NEO4J_KEY", password)
            self.graph = Graph(uri, auth=(user, password))
            # æµ‹è¯•è¿æ¥
            self.graph.run("RETURN 1")
            print("âœ… Neo4jå›¾æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Neo4jè¿æ¥å¤±è´¥: {e}")
            raise e
        
        # åŠ è½½å®ä½“ç±»å‹æ˜ å°„è¡¨
        vocab_path = "/root/KG/DeepKE/example/ner/prepare-data/vocab_dict.csv"
        if os.path.exists(vocab_path):
            self.entity_type_map = self.load_entity_types(vocab_path)
        else:
            print(f"âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}ï¼Œä½¿ç”¨é»˜è®¤å®ä½“ç±»å‹")
            self.entity_type_map = {}

        self.relation_dict={
            "rely":"ä¾èµ–",
            "b-rely":"è¢«ä¾èµ–",
            "belg":"åŒ…å«",
            "b-belg":"è¢«åŒ…å«",
            "syno":"åŒä¹‰",
            "relative":"ç›¸å¯¹",
            "attr":"æ‹¥æœ‰",
            "b-attr":"å±æ€§",
            "none":"æ— "
        }

    def clean_database(self):
        """å½»åº•æ¸…ç†æ•°æ®åº“ï¼šåˆ é™¤æ‰€æœ‰èŠ‚ç‚¹ã€å…³ç³»å’Œæ ‡ç­¾å®šä¹‰"""
        try:
            # å…ˆåˆ é™¤æ‰€æœ‰ç´¢å¼•å’Œçº¦æŸï¼ˆè§£å†³ç´¢å¼•å·²å­˜åœ¨çš„é—®é¢˜ï¼‰
            self.graph.run("DROP CONSTRAINT entity_name_unique IF EXISTS")
            self.graph.run("DROP INDEX entity_name_index IF EXISTS")
            self.graph.run("DROP INDEX entity_type_index IF EXISTS")
            
            # åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            self.graph.run("MATCH (n) DETACH DELETE n")
            print("ğŸ§¹ğŸ§¹ æ•°æ®åº“å·²å½»åº•æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœåˆ é™¤ç´¢å¼•å¤±è´¥ï¼Œåªåˆ é™¤èŠ‚ç‚¹
            print("âš ï¸ å°è¯•å¤‡é€‰æ¸…ç†æ–¹æ¡ˆ...")
            self.graph.run("MATCH (n) DETACH DELETE n")

    def load_entity_types(self, vocab_path):
        """åŠ è½½å®ä½“ç±»å‹æ˜ å°„è¡¨"""
        entity_type_map = {}
        with open(vocab_path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split(',')
                if len(parts) >= 2:
                    entity = parts[0].strip()
                    entity_type = parts[1].strip()
                    entity_type_map[entity] = entity_type
        return entity_type_map

    def normalize_entity(self, entity):
        """æ ‡å‡†åŒ–å®ä½“åç§°ï¼Œæå–æ ¸å¿ƒæœ¯è¯­"""
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºæˆ–None
        if not entity or not str(entity).strip():
            return None
            
        # ç§»é™¤æè¿°æ€§æ–‡æœ¬å’Œç‰¹æ®Šå­—ç¬¦
        patterns = [
            r'æ˜¯[\u4e00-\u9fa5]+çš„[\u4e00-\u9fa5]+',
            r'æŒ‡[\u4e00-\u9fa5]+',
            r'é€šè¿‡[\u4e00-\u9fa5]+',
            r'åˆ©ç”¨[\u4e00-\u9fa5]+',
            r'ä»[\u4e00-\u9fa5]+'
        ]
        
        for pattern in patterns:
            entity = re.sub(pattern, '', entity)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        entity = re.sub(r'[^\w\u4e00-\u9fa5]+', ' ', entity).strip()
        entity = re.sub(r'\s+', ' ', entity)
        
        # å¦‚æœå¤„ç†åä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¿”å›None
        if not entity or len(entity.strip()) == 0:
            return None
            
        return entity

    def get_entity_type(self, entity):
        """æ ¹æ®å®ä½“åç§°è·å–ç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        if entity in self.entity_type_map:
            return self.entity_type_map[entity]
        
        # å¦‚æœä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œæ ¹æ®åç§°ç‰¹å¾åˆ¤æ–­
        if 'æ’åº' in entity or 'æŸ¥æ‰¾' in entity or 'æœç´¢' in entity or 'ç®—æ³•' in entity:
            return "ARI"
        return "CON"

    def load_data(self):
        """åŠ è½½å¹¶å¤„ç†CSVæ•°æ®ï¼Œä¿ç•™åŸå§‹å…³ç³»åç§°ï¼ˆåŒ…æ‹¬b-å‰ç¼€ï¼‰"""
        file_path = os.path.join(self.data_path, "predictions.csv")
        df = pd.read_csv(file_path)
        
        # å®ä½“æ ‡å‡†åŒ–
        df['head_clean'] = df['head'].apply(self.normalize_entity)
        df['tail_clean'] = df['tail'].apply(self.normalize_entity)
        
        # è¿‡æ»¤æ— æ•ˆå…³ç³»ï¼šç§»é™¤noneå…³ç³»å’ŒåŒ…å«ç©ºå®ä½“çš„è®°å½•
        df_filtered = df[
            (df['relation'] != 'none') & 
            (df['head_clean'].notna()) & 
            (df['tail_clean'].notna()) &
            (df['head_clean'] != '') &
            (df['tail_clean'] != '')
        ]
        
        print(f"ğŸ“Š æ•°æ®è¿‡æ»¤: åŸå§‹ {len(df)} æ¡ -> æœ‰æ•ˆ {len(df_filtered)} æ¡")
        return df_filtered
    
    def load_json_data(self, json_file_path):
        """åŠ è½½å¹¶å¤„ç†JSONæ ¼å¼çš„çŸ¥è¯†å›¾è°±æ•°æ®"""
        relations_data = []
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½JSONæ–‡ä»¶: {json_file_path}")
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    sentence = data.get('sentText', '')
                    
                    for relation in data.get('relationMentions', []):
                        head_entity = relation.get('em1Text', '')
                        tail_entity = relation.get('em2Text', '')
                        relation_type = relation.get('label', '')
                        
                        # å®ä½“æ ‡å‡†åŒ–
                        head_clean = self.normalize_entity(head_entity)
                        tail_clean = self.normalize_entity(tail_entity)
                        
                        # è¿‡æ»¤æ— æ•ˆå…³ç³»
                        if (relation_type != 'none' and 
                            head_clean and tail_clean and 
                            head_clean.strip() and tail_clean.strip()):
                            
                            # ä»JSONæ•°æ®ä¸­æå–ç½®ä¿¡åº¦ä¿¡æ¯
                            confidence = relation.get('confidence', 1.0)
                            
                            relations_data.append({
                                'sentence': sentence,
                                'head': head_entity,
                                'tail': tail_entity,
                                'relation': relation_type,
                                'head_clean': head_clean,
                                'tail_clean': tail_clean,
                                'confidence': confidence  # ä½¿ç”¨å®é™…çš„ç½®ä¿¡åº¦å€¼
                            })
                            
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡Œå¤„ç†é”™è¯¯: {e}")
                    continue
        
        df = pd.DataFrame(relations_data)
        print(f"ğŸ“Š JSONæ•°æ®åŠ è½½å®Œæˆ: å…± {len(df)} æ¡æœ‰æ•ˆå…³ç³»")
        return df

    def create_nodes(self, df):
        """åˆ›å»ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹"""
        print("ğŸ”„ğŸ”„ æ­£åœ¨åˆ›å»ºå®ä½“èŠ‚ç‚¹...")
        
        # è·å–æ‰€æœ‰å”¯ä¸€å®ä½“
        all_entities = set(df['head_clean'].tolist() + df['tail_clean'].tolist())
        
        # æ‰¹é‡åˆ›å»ºèŠ‚ç‚¹
        tx = self.graph.begin()
        nodes = {}
        
        for entity in tqdm(all_entities, desc="åˆ›å»ºèŠ‚ç‚¹"):
            # è·å–å®ä½“ç±»å‹
            entity_type = self.get_entity_type(entity)
            
            # åˆ›å»ºå¸¦æ ‡ç­¾çš„èŠ‚ç‚¹
            node = Node("Entity", name=entity, type=entity_type)
            nodes[entity] = node
            tx.create(node)
        
        self.graph.commit(tx)
        print(f"âœ… æˆåŠŸåˆ›å»º {len(nodes)} ä¸ªå®ä½“èŠ‚ç‚¹")
        return nodes

    def deduplicate_relationships(self, df):
        """å»é™¤é‡å¤å…³ç³»ï¼ŒæŒ‰ç½®ä¿¡åº¦ä¿ç•™æœ€é«˜çš„å…³ç³»,å¹¶ä¸”è¿‡æ»¤æ‰ç½®ä¿¡åº¦å°äº0.8çš„å…³ç³»"""
        print("ğŸ”„ğŸ”„ æ­£åœ¨å»é‡å…³ç³»...")
        
        # è¿‡æ»¤æ‰ç½®ä¿¡åº¦å°äº0.8çš„å…³ç³»
        df = df[df['confidence'] >= self.confidence].copy()

        #è¿‡æ»¤æ‰å¤´å°¾å®ä½“ä¸ºç©ºå’Œç›¸åŒçš„å…³ç³»
        df = df[(df['head_clean'].notna()) & (df['tail_clean'].notna()) & (df['head_clean'] != '') & (df['tail_clean'] != '') & (df['head_clean'] != df['tail_clean'])].copy()

        
        # è¿‡æ»¤æ‰å…³ç³»ä¸ºç©ºçš„å…³ç³»
        df = df[df['relation'].notna() & (df['relation'] != '')].copy()

        
        # è¿‡æ»¤æ‰å…³ç³»ä¸ºnoneçš„å…³ç³»
        df = df[df['relation'] != 'none'].copy()
        
        
        # åˆ›å»ºå…³ç³»å”¯ä¸€æ ‡è¯†ï¼šå¤´å®ä½“-å°¾å®ä½“-å…³ç³»ç±»å‹
        df['relation_key'] = df['head_clean'] + '|' + df['tail_clean'] + '|' + df['relation']
        
        # æŒ‰å…³ç³»å”¯ä¸€æ ‡è¯†åˆ†ç»„ï¼Œä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„è®°å½•
        df_dedup = df.loc[df.groupby('relation_key')['confidence'].idxmax()]
        
        # å¤„ç†äº’ç›¸æŒ‡å‘çš„ç›¸åŒå…³ç³»ç±»å‹ï¼ˆA->B å’Œ B->A çš„åŒç±»å…³ç³»ï¼‰ï¼Œå…è®¸ä¸åŒå…³ç³»ç±»å‹çš„ç›¸äº’æŒ‡å‘
        mutual_relations = []
        processed_pairs = set()
        
        for _, row in df_dedup.iterrows():
            head, tail, relation = row['head_clean'], row['tail_clean'], row['relation']
            # åˆ›å»ºåŒ…å«å…³ç³»ç±»å‹çš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºè¯†åˆ«ç›¸åŒå…³ç³»ç±»å‹çš„äº’ç›¸æŒ‡å‘
            pair_key = tuple(sorted([head, tail]) + [relation])
            
            if pair_key in processed_pairs:
                continue
                
            # æŸ¥æ‰¾äº’ç›¸æŒ‡å‘çš„ç›¸åŒå…³ç³»ç±»å‹
            reverse_relation = df_dedup[
                (df_dedup['head_clean'] == tail) & 
                (df_dedup['tail_clean'] == head) & 
                (df_dedup['relation'] == relation)
            ]
            
            if not reverse_relation.empty:
                # å­˜åœ¨äº’ç›¸æŒ‡å‘çš„ç›¸åŒå…³ç³»ç±»å‹ï¼Œä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„
                current_confidence = row['confidence']
                reverse_confidence = reverse_relation.iloc[0]['confidence']
                
                if current_confidence >= reverse_confidence:
                    mutual_relations.append(row)
                else:
                    mutual_relations.append(reverse_relation.iloc[0])
                    
                processed_pairs.add(pair_key)
            else:
                # æ²¡æœ‰äº’ç›¸æŒ‡å‘çš„ç›¸åŒå…³ç³»ç±»å‹ï¼Œç›´æ¥ä¿ç•™
                mutual_relations.append(row)
        
        result_df = pd.DataFrame(mutual_relations)
        print(f"ğŸ“Š å»é‡å‰: {len(df)} æ¡å…³ç³»ï¼Œå»é‡å: {len(result_df)} æ¡å…³ç³»")
        return result_df
    
    def create_relationships(self, df, nodes):
        """åˆ›å»ºå®ä½“é—´çš„å…³ç³»ï¼Œä½¿ç”¨åŸå§‹å…³ç³»åç§°ï¼ˆåŒ…æ‹¬b-å‰ç¼€ï¼‰"""
        print("ğŸ”„ğŸ”„ æ­£åœ¨åˆ›å»ºå…³ç³»...")
        
        # å…ˆè¿›è¡Œå…³ç³»å»é‡
        df_dedup = self.deduplicate_relationships(df)
        
        tx = self.graph.begin()
        relationship_types = set()
        
        for _, row in tqdm(df_dedup.iterrows(), total=len(df_dedup), desc="åˆ›å»ºå…³ç³»"):
            source = nodes.get(row['head_clean'])
            target = nodes.get(row['tail_clean'])
            
            if not source or not target:
                continue
                
            # åˆ›å»ºå…³ç³»ï¼Œä½¿ç”¨ä¸­æ–‡å…³ç³»åç§°
            rel_type = self.relation_dict.get(row['relation'], row['relation'])
            relationship = Relationship(source, rel_type, target, 
                                      confidence=row['confidence'],
                                      source_sentence=row['sentence'])
            tx.create(relationship)
            relationship_types.add(rel_type)
        
        self.graph.commit(tx)
        print(f"âœ… æˆåŠŸåˆ›å»º {len(df_dedup)} ä¸ªå…³ç³»ï¼ŒåŒ…å« {len(relationship_types)} ç§å…³ç³»ç±»å‹")
        
    def create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            existing_indexes = self.graph.run("SHOW INDEXES").data()
            index_names = [index['name'] for index in existing_indexes]
            
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨åˆ™åˆ›å»º
            if "entity_name_unique" not in index_names:
                self.graph.run("CREATE CONSTRAINT entity_name_unique FOR (e:Entity) REQUIRE e.name IS UNIQUE")
            
            if "entity_type_index" not in index_names:
                self.graph.run("CREATE INDEX entity_type_index FOR (e:Entity) ON (e.type)")
            
            print("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
        except Exception as e:
            print(f"åˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {e}")
            # å°è¯•åˆ›å»ºç´¢å¼•è€Œä¸æ£€æŸ¥ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
            try:
                self.graph.run("CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE")
                self.graph.run("CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)")
                print("âœ… å¤‡é€‰æ–¹æ¡ˆç´¢å¼•åˆ›å»ºå®Œæˆ")
            except Exception as e2:
                print(f"å¤‡é€‰æ–¹æ¡ˆåˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {e2}")

    def build_knowledge_graph(self, data_source='csv', json_file_path=None, csv_file_path=None):
        """
        æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            data_source (str): æ•°æ®æºç±»å‹ï¼Œ'csv' æˆ– 'json'
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„ï¼ˆå½“data_source='json'æ—¶å¿…éœ€ï¼‰
            csv_file_path (str): CSVæ–‡ä»¶è·¯å¾„ï¼ˆå½“data_source='csv'ä¸”æŒ‡å®šæ–‡ä»¶æ—¶å¿…éœ€ï¼‰
        """
        # å½»åº•æ¸…ç†æ•°æ®åº“
        self.clean_database()
        
        # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
        if data_source == 'json':
            if not json_file_path:
                raise ValueError("ä½¿ç”¨JSONæ•°æ®æºæ—¶å¿…é¡»æä¾›json_file_pathå‚æ•°")
            if not os.path.exists(json_file_path):
                raise FileNotFoundError(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
            df = self.load_json_data(json_file_path)
        else:
            if csv_file_path:
                if not os.path.exists(csv_file_path):
                    raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
                df = pd.read_csv(csv_file_path)
                # æ ‡å‡†åŒ–åˆ—å
                if 'head' in df.columns and 'head_clean' not in df.columns:
                    df['head_clean'] = df['head']
                if 'tail' in df.columns and 'tail_clean' not in df.columns:
                    df['tail_clean'] = df['tail']
            else:
                df = self.load_data()
            
        print(f"ğŸ“ŠğŸ“Š å·²åŠ è½½ {len(df)} æ¡çŸ¥è¯†è®°å½•")
        
        # åˆ›å»ºèŠ‚ç‚¹å’Œå…³ç³»
        nodes = self.create_nodes(df)
        self.create_relationships(df, nodes)
        
        # åˆ›å»ºç´¢å¼•
        self.create_indexes()
        
        # éªŒè¯å›¾ç»“æ„
        try:
            result = self.graph.run("MATCH (n) RETURN count(n) AS node_count").data()
            print(f"ğŸ“ˆğŸ“ˆ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼åŒ…å« {result[0]['node_count']} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            print(f"éªŒè¯å›¾ç»“æ„æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    
    kg_builder = Neo4jKnowledgeGraph()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--json' and len(sys.argv) > 2:
            # ä½¿ç”¨JSONæ•°æ®æº
            json_file_path = sys.argv[2]
            print(f"ğŸ”„ ä½¿ç”¨JSONæ•°æ®æº: {json_file_path}")
            kg_builder.build_knowledge_graph(data_source='json', json_file_path=json_file_path)
        elif sys.argv[1] == '--csv' and len(sys.argv) > 2:
            # ä½¿ç”¨æŒ‡å®šçš„CSVæ•°æ®æº
            csv_file_path = sys.argv[2]
            print(f"ğŸ”„ ä½¿ç”¨CSVæ•°æ®æº: {csv_file_path}")
            kg_builder.build_knowledge_graph(data_source='csv', csv_file_path=csv_file_path)
        else:
            print("ç”¨æ³•: python product.py [--json <json_file_path>] [--csv <csv_file_path>]")
            print("ç¤ºä¾‹: python product.py --json /path/to/iteration_version_1.json")
            print("ç¤ºä¾‹: python product.py --csv /path/to/predictions.csv")
    else:
        # é»˜è®¤ä½¿ç”¨CSVæ•°æ®æº
        print("ğŸ”„ ä½¿ç”¨é»˜è®¤CSVæ•°æ®æº")
        kg_builder.build_knowledge_graph()