from py2neo import Graph, Node, Relationship
import os
import pandas as pd
import re
from tqdm import tqdm

class Neo4jKnowledgeGraph:
    def __init__(self):
        """åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“è¿æ¥"""
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        cur_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_path = os.path.join(cur_dir, "data")
        
        # Neo4jè¿æ¥é…ç½®
        self.graph = Graph("bolt://localhost:7687", auth=("neo4j", "055625540"))
        
        # åŠ è½½å®ä½“ç±»å‹æ˜ å°„è¡¨
        self.entity_type_map = self.load_entity_types("/root/KG/DeepKE/example/ner/prepare-data/vocab_dict.csv")
        print("âœ… Neo4jå›¾æ•°æ®åº“è¿æ¥æˆåŠŸ")

    def clean_database(self):
        """å½»åº•æ¸…ç†æ•°æ®åº“ï¼šåˆ é™¤æ‰€æœ‰èŠ‚ç‚¹ã€å…³ç³»å’Œæ ‡ç­¾å®šä¹‰"""
        try:
            # å…ˆåˆ é™¤æ‰€æœ‰ç´¢å¼•å’Œçº¦æŸï¼ˆè§£å†³ç´¢å¼•å·²å­˜åœ¨çš„é—®é¢˜ï¼‰
            self.graph.run("DROP CONSTRAINT entity_name_unique IF EXISTS")
            self.graph.run("DROP INDEX entity_name_index IF EXISTS")
            self.graph.run("DROP INDEX entity_type_index IF EXISTS")
            
            # åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            self.graph.run("MATCH (n) DETACH DELETE n")
            print("ğŸ§¹ğŸ§¹ æ•°æ®åº“å·²å½»åº•æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœåˆ é™¤ç´¢å¼•å¤±è´¥ï¼Œåªåˆ é™¤èŠ‚ç‚¹
            print("âš ï¸ å°è¯•å¤‡é€‰æ¸…ç†æ–¹æ¡ˆ...")
            self.graph.run("MATCH (n) DETACH DELETE n")

    def load_entity_types(self, vocab_path):
        """åŠ è½½å®ä½“ç±»å‹æ˜ å°„è¡¨"""
        entity_type_map = {}
        with open(vocab_path, "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split(',')
                if len(parts) >= 2:
                    entity = parts[0].strip()
                    entity_type = parts[1].strip()
                    entity_type_map[entity] = entity_type
        return entity_type_map

    def normalize_entity(self, entity):
        """æ ‡å‡†åŒ–å®ä½“åç§°ï¼Œæå–æ ¸å¿ƒæœ¯è¯­"""
        # ç§»é™¤æè¿°æ€§æ–‡æœ¬å’Œç‰¹æ®Šå­—ç¬¦
        patterns = [
            r'æ˜¯[\u4e00-\u9fa5]+çš„[\u4e00-\u9fa5]+',
            r'æŒ‡[\u4e00-\u9fa5]+',
            r'é€šè¿‡[\u4e00-\u9fa5]+',
            r'åˆ©ç”¨[\u4e00-\u9fa5]+',
            r'ä»[\u4e00-\u9fa5]+'
        ]
        
        for pattern in patterns:
            entity = re.sub(pattern, '', entity)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        entity = re.sub(r'[^\w\u4e00-\u9fa5]+', ' ', entity).strip()
        entity = re.sub(r'\s+', ' ', entity)
        
        return entity

    def get_entity_type(self, entity):
        """æ ¹æ®å®ä½“åç§°è·å–ç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦åœ¨è¯æ±‡è¡¨ä¸­
        if entity in self.entity_type_map:
            return self.entity_type_map[entity]
        
        # å¦‚æœä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œæ ¹æ®åç§°ç‰¹å¾åˆ¤æ–­
        if 'æ’åº' in entity or 'æŸ¥æ‰¾' in entity or 'æœç´¢' in entity or 'ç®—æ³•' in entity:
            return "ARI"
        return "CON"

    def load_data(self):
        """åŠ è½½å¹¶å¤„ç†CSVæ•°æ®ï¼Œä¿ç•™åŸå§‹å…³ç³»åç§°ï¼ˆåŒ…æ‹¬b-å‰ç¼€ï¼‰"""
        file_path = os.path.join(self.data_path, "predictions.csv")
        df = pd.read_csv(file_path)
        
        # å®ä½“æ ‡å‡†åŒ–
        df['head_clean'] = df['head'].apply(self.normalize_entity)
        df['tail_clean'] = df['tail'].apply(self.normalize_entity)
        
        # è¿‡æ»¤æ— æ•ˆå…³ç³»ï¼ˆåªç§»é™¤noneå…³ç³»ï¼‰
        return df[df['relation'] != 'none']

    def create_nodes(self, df):
        """åˆ›å»ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹"""
        print("ğŸ”„ğŸ”„ æ­£åœ¨åˆ›å»ºå®ä½“èŠ‚ç‚¹...")
        
        # è·å–æ‰€æœ‰å”¯ä¸€å®ä½“
        all_entities = set(df['head_clean'].tolist() + df['tail_clean'].tolist())
        
        # æ‰¹é‡åˆ›å»ºèŠ‚ç‚¹
        tx = self.graph.begin()
        nodes = {}
        
        for entity in tqdm(all_entities, desc="åˆ›å»ºèŠ‚ç‚¹"):
            # è·å–å®ä½“ç±»å‹
            entity_type = self.get_entity_type(entity)
            
            # åˆ›å»ºå¸¦æ ‡ç­¾çš„èŠ‚ç‚¹
            node = Node("Entity", name=entity, type=entity_type)
            nodes[entity] = node
            tx.create(node)
        
        self.graph.commit(tx)
        print(f"âœ… æˆåŠŸåˆ›å»º {len(nodes)} ä¸ªå®ä½“èŠ‚ç‚¹")
        return nodes

    def create_relationships(self, df, nodes):
        """åˆ›å»ºå®ä½“é—´çš„å…³ç³»ï¼Œä½¿ç”¨åŸå§‹å…³ç³»åç§°ï¼ˆåŒ…æ‹¬b-å‰ç¼€ï¼‰"""
        print("ğŸ”„ğŸ”„ æ­£åœ¨åˆ›å»ºå…³ç³»...")
        
        tx = self.graph.begin()
        relationship_types = set()
        
        for _, row in tqdm(df.iterrows(), total=len(df), desc="åˆ›å»ºå…³ç³»"):
            source = nodes.get(row['head_clean'])
            target = nodes.get(row['tail_clean'])
            
            if not source or not target:
                continue
                
            # åˆ›å»ºå…³ç³»ï¼Œä½¿ç”¨åŸå§‹å…³ç³»åç§°ï¼ˆåŒ…æ‹¬b-å‰ç¼€ï¼‰
            rel_type = row['relation']
            relationship = Relationship(source, rel_type, target, 
                                      confidence=row['confidence'],
                                      source_sentence=row['sentence'])
            tx.create(relationship)
            relationship_types.add(rel_type)
        
        self.graph.commit(tx)
        print(f"âœ… æˆåŠŸåˆ›å»º {len(df)} ä¸ªå…³ç³»ï¼ŒåŒ…å« {len(relationship_types)} ç§å…³ç³»ç±»å‹")
        
    def create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
            existing_indexes = self.graph.run("SHOW INDEXES").data()
            index_names = [index['name'] for index in existing_indexes]
            
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨åˆ™åˆ›å»º
            if "entity_name_unique" not in index_names:
                self.graph.run("CREATE CONSTRAINT entity_name_unique FOR (e:Entity) REQUIRE e.name IS UNIQUE")
            
            if "entity_type_index" not in index_names:
                self.graph.run("CREATE INDEX entity_type_index FOR (e:Entity) ON (e.type)")
            
            print("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
        except Exception as e:
            print(f"åˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {e}")
            # å°è¯•åˆ›å»ºç´¢å¼•è€Œä¸æ£€æŸ¥ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
            try:
                self.graph.run("CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE")
                self.graph.run("CREATE INDEX entity_type_index IF NOT EXISTS FOR (e:Entity) ON (e.type)")
                print("âœ… å¤‡é€‰æ–¹æ¡ˆç´¢å¼•åˆ›å»ºå®Œæˆ")
            except Exception as e2:
                print(f"å¤‡é€‰æ–¹æ¡ˆåˆ›å»ºç´¢å¼•æ—¶å‡ºé”™: {e2}")

    def build_knowledge_graph(self):
        """æ„å»ºçŸ¥è¯†å›¾è°±ä¸»æµç¨‹"""
        # å½»åº•æ¸…ç†æ•°æ®åº“
        self.clean_database()
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        df = self.load_data()
        print(f"ğŸ“ŠğŸ“Š å·²åŠ è½½ {len(df)} æ¡çŸ¥è¯†è®°å½•")
        
        # åˆ›å»ºèŠ‚ç‚¹å’Œå…³ç³»
        nodes = self.create_nodes(df)
        self.create_relationships(df, nodes)
        
        # åˆ›å»ºç´¢å¼•
        self.create_indexes()
        
        # éªŒè¯å›¾ç»“æ„
        try:
            result = self.graph.run("MATCH (n) RETURN count(n) AS node_count").data()
            print(f"ğŸ“ˆğŸ“ˆ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼åŒ…å« {result[0]['node_count']} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            print(f"éªŒè¯å›¾ç»“æ„æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    kg_builder = Neo4jKnowledgeGraph()
    kg_builder.build_knowledge_graph()