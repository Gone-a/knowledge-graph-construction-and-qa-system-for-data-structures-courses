import os
import csv
import time
import json
import threading
import concurrent.futures
from tqdm import tqdm
from openai import OpenAI
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='relation_processing.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ä¸¥æ ¼éµå¾ªç”¨æˆ·æä¾›çš„7ç§å…³ç³»ç±»å‹
RELATION_TYPES = [
    "rely", "b-rely",  # ä¾èµ–/è¢«ä¾èµ–
    "belg", "b-belg",  # åŒ…å«/å±äº
    "syno", "relative",  # åŒä¹‰/ç›¸å¯¹ï¼ˆå–ä»£antoï¼‰
    "attr", "b-attr",  # æ‹¥æœ‰/å±æ€§
    "none"             # æ— å…³ç³»
]

# ä¾èµ–å…³ç³»å…³é”®è¯ï¼ˆç”¨äºåå¤„ç†ä¿®æ­£ï¼‰
DEPENDENCY_KEYWORDS = {"ä¾èµ–", "å–å†³äº", "éœ€è¦", "åŸºäº", "åˆ©ç”¨", "è¦æ±‚"}

# ç”¨æˆ·æä¾›çš„è¯¦ç»†è§„åˆ™æè¿°ï¼ˆä¼˜åŒ–åçš„æç¤ºè¯ï¼‰
RULES_DESCRIPTION = """
è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™åˆ†æå…³ç³»ç±»å‹ï¼š
1. **ä¾èµ–å…³ç³» (rely)**: å®ä½“å­˜åœ¨æ˜æ˜¾çš„é€»è¾‘é¡ºåºå…³ç³»ï¼Œheadçš„å­¦ä¹ æˆ–åŠŸèƒ½ä¾èµ–äºtail 
   - ç¤ºä¾‹: "å“ˆå¸ŒæŸ¥æ‰¾çš„æ•ˆç‡ä¾èµ–äºå“ˆå¸Œå‡½æ•°" â†’ (å“ˆå¸ŒæŸ¥æ‰¾æ•ˆç‡, å“ˆå¸Œå‡½æ•°)=rely
2. **è¢«ä¾èµ–å…³ç³» (b-rely)**: å®ä½“å­˜åœ¨æ˜æ˜¾çš„é€»è¾‘é¡ºåºå…³ç³»ï¼Œheadè¢«tailä¾èµ– 
   - ç¤ºä¾‹: "æ ˆæ˜¯å®ç°å‡½æ•°è°ƒç”¨çš„åŸºç¡€" â†’ (å‡½æ•°è°ƒç”¨, æ ˆ)=b-rely
3. **åŒ…å«å…³ç³» (belg)**: tailä½œä¸ºæ¦‚å¿µèŒƒç•´åŒ…å«head (æ•´ä½“-éƒ¨åˆ†å…³ç³»)
   - ç¤ºä¾‹: "äºŒå‰æ ‘ç”±æ ¹èŠ‚ç‚¹ç»„æˆ" â†’ (äºŒå‰æ ‘, æ ¹èŠ‚ç‚¹)=belg
4. **å±äºå…³ç³» (b-belg)**: headå±äºtailçš„æ¦‚å¿µèŒƒç•´ (is-aå…³ç³»)
   - ç¤ºä¾‹: "AVLæ ‘æ˜¯ä¸€ç§è‡ªå¹³è¡¡äºŒå‰æœç´¢æ ‘" â†’ (AVLæ ‘, äºŒå‰æœç´¢æ ‘)=b-belg
5. **åŒä¹‰å…³ç³» (syno)**: headå’Œtailåœ¨ä¸åŒå«æ³•ä¸‹æŒ‡å‘ç›¸åŒæ¦‚å¿µ
   - ç¤ºä¾‹: "å“ˆå¸Œè¡¨ä¹Ÿå«æ•£åˆ—è¡¨" â†’ (å“ˆå¸Œè¡¨, æ•£åˆ—è¡¨)=syno
6. **ç›¸å¯¹å…³ç³» (relative)**: headå’Œtailåœ¨åŠŸèƒ½ä¸Šå½¢æˆäº’è¡¥å¯¹ç«‹
   - ç¤ºä¾‹: "æ·±åº¦ä¼˜å…ˆæœç´¢ä¸å¹¿åº¦ä¼˜å…ˆæœç´¢æ˜¯å›¾éå†çš„ä¸¤ç§åŸºæœ¬æ–¹æ³•" â†’ (æ·±åº¦ä¼˜å…ˆæœç´¢, å¹¿åº¦ä¼˜å…ˆæœç´¢)=relative
7. **æ‹¥æœ‰å…³ç³» (attr)**: tailæ˜¯æè¿°headçš„å±æ€§å®ä½“
   - ç¤ºä¾‹: "æ•°ç»„å…·æœ‰å›ºå®šé•¿åº¦ç‰¹æ€§" â†’ (æ•°ç»„, å›ºå®šé•¿åº¦)=attr
8. **å±æ€§å…³ç³» (b-attr)**: headæ˜¯æè¿°tailçš„å±æ€§å®ä½“
   - ç¤ºä¾‹: "æ—¶é—´å¤æ‚åº¦æ˜¯åˆ†æç®—æ³•æ•ˆç‡çš„é‡è¦æŒ‡æ ‡" â†’ (æ—¶é—´å¤æ‚åº¦, ç®—æ³•æ•ˆç‡)=b-attr
9. **æ— å…³ç³» (none)**: headå’Œtailä¸å­˜åœ¨ç›´æ¥è¯­ä¹‰å…³è”
   - ç¤ºä¾‹: "çº¢é»‘æ ‘å’Œå¿«é€Ÿæ’åºéƒ½æ˜¯å¸¸ç”¨ç®—æ³•" â†’ (çº¢é»‘æ ‘, å¿«é€Ÿæ’åº)=none

ä¸¥æ ¼æ³¨æ„äº‹é¡¹ï¼š
- åªèƒ½ä½¿ç”¨ä»¥ä¸Š9ç§å…³ç³»ç±»å‹
- è¦†ç›–æ•°æ®ç»“æ„ä¸ç®—æ³•å…¨é¢†åŸŸï¼šçº¿æ€§ç»“æ„ã€æ ‘ç»“æ„ã€å›¾ç»“æ„ã€ç®—æ³•è®¾è®¡ç­‰
- é‡ç‚¹åˆ†æä¸‰ç§æ ¸å¿ƒå…³ç³»ï¼š
  a) æ•°æ®ç»“æ„ç»„ä»¶é—´å…³ç³»ï¼ˆå¦‚èŠ‚ç‚¹-è¾¹ï¼‰
  b) ç®—æ³•ä¸å®ç°æŠ€æœ¯å…³ç³»ï¼ˆå¦‚æ’åº-åˆ†æ²»ï¼‰
  c) æ€§èƒ½æŒ‡æ ‡å…³è”ï¼ˆæ—¶é—´å¤æ‚åº¦-ç©ºé—´å¤æ‚åº¦ï¼‰
"""

# ç¼“å­˜å®ç°ï¼ˆå‡å°‘é‡å¤APIè°ƒç”¨ï¼‰
class RelationCache:
    def __init__(self):
        self.cache = {}
        self.lock = threading.Lock()
    
    def get(self, key):
        """è·å–ç¼“å­˜ç»“æœ"""
        with self.lock:
            return self.cache.get(key)
    
    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        with self.lock:
            self.cache[key] = value

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
def create_client():
    return OpenAI(
        base_url="https://ark.cn-beijing.volces.com/api/v3",
        api_key=os.environ.get("ARK_API_KEY"),
        timeout=60  # 60ç§’è¶…æ—¶
    )

# ä¼˜åŒ–åçš„æç¤ºè¯ç”Ÿæˆï¼ˆå¢åŠ ç¤ºä¾‹å’Œæ ¼å¼çº¦æŸï¼‰
def generate_relation_prompt(item):
    """ç”ŸæˆåŸºäºè§„åˆ™å’Œç¤ºä¾‹çš„æç¤ºè¯"""
    return f"""
# å…³ç³»åˆ†æä»»åŠ¡
{RULES_DESCRIPTION}

## è¾“å‡ºè¦æ±‚
- åªè¾“å‡ºå…³ç³»ç±»å‹åç§°ï¼ˆå¦‚ï¼šrelyï¼‰
- ç¦æ­¢æ·»åŠ è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·
- è¾“å‡ºå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹åç§°ï¼š{", ".join(RELATION_TYPES)}

## ç¤ºä¾‹åˆ†æ
1. å¥å­: "æ ˆå’Œé˜Ÿåˆ—éƒ½æ˜¯çº¿æ€§æ•°æ®ç»“æ„"
   å®ä½“: ("æ ˆ", "é˜Ÿåˆ—") â†’ å…³ç³»: none
2. å¥å­: "äºŒå‰æ ‘ç”±æ ¹èŠ‚ç‚¹å’Œå­èŠ‚ç‚¹ç»„æˆ"
   å®ä½“: ("äºŒå‰æ ‘", "æ ¹èŠ‚ç‚¹") â†’ å…³ç³»: belg
3. å¥å­: "å“ˆå¸Œè¡¨çš„æŸ¥æ‰¾æ•ˆç‡å–å†³äºå“ˆå¸Œå‡½æ•°çš„è´¨é‡"
   å®ä½“: ("æŸ¥æ‰¾æ•ˆç‡", "å“ˆå¸Œå‡½æ•°") â†’ å…³ç³»: rely
4. å¥å­: "é˜Ÿåˆ—æ˜¯å…ˆè¿›å…ˆå‡ºçš„æ•°æ®ç»“æ„"
   å®ä½“: ("é˜Ÿåˆ—", "å…ˆè¿›å…ˆå‡º") â†’ å…³ç³»: attr
5. å¥å­: "æ·±åº¦ä¼˜å…ˆæœç´¢å’Œå¹¿åº¦ä¼˜å…ˆæœç´¢æ˜¯å›¾éå†çš„ä¸¤ç§æ–¹æ³•"
   å®ä½“: ("æ·±åº¦ä¼˜å…ˆæœç´¢", "å¹¿åº¦ä¼˜å…ˆæœç´¢") â†’ å…³ç³»: relative

## å¾…åˆ†æå†…å®¹
å¥å­: "{item['sentence']}"
å®ä½“å¯¹: ("{item['head']}", "{item['tail']}")
å…³ç³»ç±»å‹:
"""

# è§£æAPIå“åº”å¹¶éªŒè¯å…³ç³»ç±»å‹
def parse_and_validate_relation(response_content):
    """è§£æå¹¶éªŒè¯APIè¿”å›çš„å…³ç³»ç±»å‹"""
    # æå–å“åº”ä¸­çš„å…³ç³»ç±»å‹
    response_content = response_content.strip()
    
    if not response_content:
        return "none"
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…æœ‰æ•ˆçš„ç±»å‹åç§°
    for relation in RELATION_TYPES:
        if relation == response_content:
            return relation
    
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    for relation in RELATION_TYPES:
        # éƒ¨åˆ†åŒ¹é…æ£€æŸ¥
        if response_content.startswith(relation[:2]):
            return relation
    
    logging.warning(f"æ— æ•ˆå…³ç³»ç±»å‹: {response_content}")
    return "none"

# ç»“æœåå¤„ç†ä¼˜åŒ–
def postprocess_relation(item, predicted_relation):
    """åŸºäºè§„åˆ™ä¿®æ­£é¢„æµ‹ç»“æœ"""
    head = item['head'].lower()
    tail = item['tail'].lower()
    sentence = item['sentence'].lower()
    
    # è§„åˆ™1: åŒç±»å®ä½“é»˜è®¤æ— å…³ç³»
    if predicted_relation == "belg" and head.split()[-1] == tail.split()[-1]:
        return "none"
    
    # è§„åˆ™2: åŒè¯æ ¹é»˜è®¤ä¸ºåŒä¹‰
    if predicted_relation == "none" and head.split('_')[0] == tail.split('_')[0]:
        return "syno"
    
    # è§„åˆ™3: ä¿®æ­£æ–¹å‘æ€§é”™è¯¯
    if predicted_relation == "rely":
        # æ£€æŸ¥å¥å­ä¸­æ˜¯å¦åŒ…å«ä¾èµ–å…³é”®è¯
        for kw in DEPENDENCY_KEYWORDS:
            if kw in sentence:
                # æ£€æŸ¥å®ä½“ä½ç½®å…³ç³»
                head_pos = sentence.find(head)
                tail_pos = sentence.find(tail)
                if head_pos != -1 and tail_pos != -1 and tail_pos < head_pos:
                    return "b-rely"
    
    return predicted_relation

# è°ƒç”¨ç«å±±å¼•æ“APIåˆ†æå…³ç³»ï¼ˆå¸¦ç¼“å­˜å’Œåå¤„ç†ï¼‰
def analyze_relation_with_retry(item, client, cache, max_retries=2):
    """å¸¦ç¼“å­˜å’Œé‡è¯•æœºåˆ¶çš„å…³ç³»åˆ†æAPIè°ƒç”¨"""
    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆä½¿ç”¨å¥å­å’Œå®ä½“å¯¹ï¼‰
    cache_key = f"{item['sentence']}||{item['head']}||{item['tail']}"
    
    # æ£€æŸ¥ç¼“å­˜
    cached_result = cache.get(cache_key)
    if cached_result:
        return cached_result
    
    # ç”Ÿæˆæç¤ºè¯
    prompt = generate_relation_prompt(item)
    attempts = 0
    
    while attempts < max_retries:
        try:
            # è°ƒç”¨API
            response = client.chat.completions.create(
                model="doubao-1-5-pro-32k-250115",
                messages=[
                    {
                        "role": "system", 
                        "content": f"ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„æ•°æ®ç»“æ„ä¸“å®¶ï¼Œä¸¥æ ¼æŒ‰ç…§è§„åˆ™åˆ†æå…³ç³»ï¼Œåªä½¿ç”¨ä»¥ä¸‹ç±»å‹: {', '.join(RELATION_TYPES)}"
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=10,
                temperature=0.0  # é›¶éšæœºæ€§ç¡®ä¿ç¨³å®šæ€§
            )
            
            # è§£æå’ŒéªŒè¯å“åº”
            response_content = response.choices[0].message.content.strip()
            relation = parse_and_validate_relation(response_content)
            
            # åå¤„ç†ä¿®æ­£
            relation = postprocess_relation(item, relation)
            
            # ç¼“å­˜ç»“æœ
            cache.set(cache_key, relation)
            return relation
            
        except Exception as e:
            logging.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)} - é‡è¯• {attempts+1}/{max_retries}")
            attempts += 1
            time.sleep(1.5 ** attempts)  # æŒ‡æ•°é€€é¿ï¼ˆé™ä½ç­‰å¾…æ—¶é—´ï¼‰
    
    logging.error(f"å…³ç³»åˆ†æå¤±è´¥: {item}")
    return "none"  # é‡è¯•å¤±è´¥åè¿”å›é»˜è®¤å€¼

# æ•°æ®é¢„å¤„ç†ä¼˜åŒ–
def preprocess_data(data):
    """è¿‡æ»¤æ— æ•ˆå®ä½“å¯¹ï¼Œå‡å°‘ä¸å¿…è¦çš„APIè°ƒç”¨"""
    valid_data = []
    skipped_count = 0
    
    for item in data:
        # è·³è¿‡ç©ºå®ä½“
        if not item['head'].strip() or not item['tail'].strip():
            skipped_count += 1
            continue
            
        # è·³è¿‡ç›¸åŒå®ä½“
        if item['head'].lower() == item['tail'].lower():
            skipped_count += 1
            continue
            
        # è·³è¿‡æ˜æ˜¾æ— å…³è”çš„å®ä½“ï¼ˆè·ç¦»è¶…è¿‡100å­—ç¬¦ï¼‰
        try:
            head_offset = int(item['head_offset'])
            tail_offset = int(item['tail_offset'])
            head_end = head_offset + len(item['head'])
            
            if abs(tail_offset - head_end) > 100:
                # ç›´æ¥æ ‡è®°ä¸ºæ— å…³ç³»ï¼Œä¸è°ƒç”¨API
                item['predicted_relation'] = "none"
                valid_data.append(item)
                skipped_count += 1
                continue
        except ValueError:
            pass  # å¦‚æœä½ç½®æ— æ•ˆï¼Œç»§ç»­å¤„ç†
        
        valid_data.append(item)
    
    logging.info(f"æ•°æ®é¢„å¤„ç†: åŸå§‹æ•°æ® {len(data)} æ¡, è¿‡æ»¤ {skipped_count} æ¡, å‰©ä½™ {len(valid_data)} æ¡")
    return valid_data

# å¤„ç†å•ä¸ªæ¡ç›®
def process_item(item, client, cache):
    """å¤„ç†å•ä¸ªæ¡ç›®ï¼Œç”Ÿæˆç¬¦åˆå›¾ç‰‡æ ¼å¼çš„è¾“å‡ºè¡Œ"""
    # å¦‚æœé¢„å¤„ç†å·²æ·»åŠ å…³ç³»ï¼Œç›´æ¥ä½¿ç”¨
    if 'predicted_relation' in item:
        relation = item['predicted_relation']
    else:
        # è°ƒç”¨APIåˆ†æå…³ç³»ï¼ˆå¸¦ç¼“å­˜ï¼‰
        relation = analyze_relation_with_retry(item, client, cache)
    
    # æŒ‰ç…§å›¾ç‰‡ä¸­çš„æ ¼å¼è¿”å›CSVè¡Œ
    return [
        item["sentence"],
        relation,
        item["head"],
        item["tail"],
        str(item["head_offset"]),
        str(item["tail_offset"])
    ]

# æ‰¹é‡å¤„ç†JSONæ–‡ä»¶å¹¶è¾“å‡ºCSV
def process_json_file(input_file, output_file, concurrency=3):  # é™ä½å¹¶å‘æ•°å‡å°‘æˆæœ¬
    """
    å¤„ç†JSONæ–‡ä»¶ï¼Œè¾“å‡ºç¬¦åˆå›¾ç‰‡æ ¼å¼çš„CSV
    :param input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
    :param output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
    :param concurrency: å¹¶å‘æ•°ï¼ˆé™ä½ä»¥å‡å°‘APIå‹åŠ›ï¼‰
    """
    # éªŒè¯APIå¯†é’¥
    if not os.environ.get("ARK_API_KEY"):
        print("âŒâŒ é”™è¯¯: è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY")
        return
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒâŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    print(f"âœ… å·²åŠ è½½ {len(data)} æ¡è®°å½•")
    
    # æ•°æ®é¢„å¤„ç†
    data = preprocess_data(data)
    print(f"âš¡ é¢„å¤„ç†åä¿ç•™ {len(data)} æ¡è®°å½•")
    
    client = create_client()
    cache = RelationCache()  # åˆ›å»ºç¼“å­˜å®ä¾‹
    processed_rows = []
    
    # æ·»åŠ CSVæ ‡é¢˜è¡Œï¼ˆä¸å›¾ç‰‡ä¸€è‡´ï¼‰
    header = ["sentence", "relation", "head", "tail", "head_offset", "tail_offset"]
    processed_rows.append(header)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        # å‡†å¤‡ä»»åŠ¡
        futures = {
            executor.submit(process_item, item, client, cache): item
            for item in data
        }
        
        # å¤„ç†ç»“æœå¸¦è¿›åº¦æ¡
        completed = tqdm(
            concurrent.futures.as_completed(futures),
            total=len(data),
            desc="åˆ†æè¯­ä¹‰å…³ç³»",
            dynamic_ncols=True
        )
        
        for future in completed:
            try:
                result = future.result()
                processed_rows.append(result)
            except Exception as e:
                logging.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                # æ·»åŠ é»˜è®¤å€¼ä½œä¸ºå›é€€
                processed_rows.append([
                    item.get("sentence", "å¤„ç†å¤±è´¥"), 
                    "none", 
                    item.get("head", "N/A"), 
                    item.get("tail", "N/A"), 
                    str(item.get("head_offset", 0)), 
                    str(item.get("tail_offset", 0))
                ])
    
    # ä¿å­˜ä¸ºCSVæ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(processed_rows)
        print(f"âœ… å¤„ç†å®Œæˆ! CSVç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ç»Ÿè®¡å…³ç³»åˆ†å¸ƒ
        relation_counts = {rel: 0 for rel in RELATION_TYPES}
        total_rows = len(processed_rows) - 1  # å‡å»æ ‡é¢˜è¡Œ
        
        if total_rows > 0:
            for row in processed_rows[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                relation = row[1]
                if relation in relation_counts:
                    relation_counts[relation] += 1
        
        print("\nğŸ“Š å…³ç³»ç±»å‹åˆ†å¸ƒç»Ÿè®¡:")
        for rel in RELATION_TYPES:
            count = relation_counts[rel]
            percent = count / total_rows * 100 if total_rows > 0 else 0
            print(f"- {rel}: {count} æ¡ ({percent:.1f}%)")
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        if hasattr(cache, 'cache'):
            cache_size = len(cache.cache)
            hit_rate = (cache_size / total_rows) * 100 if total_rows > 0 else 0
            print(f"\nğŸ’¾ ç¼“å­˜æ•ˆæœ: ç¼“å­˜äº† {cache_size} ä¸ªç»“æœ, ç¼“å­˜å‘½ä¸­ç‡ {hit_rate:.1f}%")
            
    except Exception as e:
        print(f"âŒâŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    # é…ç½®è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„
    input_file = "train_predict.json"      # æ›¿æ¢ä¸ºæ‚¨çš„è¾“å…¥æ–‡ä»¶è·¯å¾„
    output_file = "optimized_relations.csv"  # è¾“å‡ºCSVæ–‡ä»¶
    
    # æ‰§è¡Œå¤„ç†ï¼ˆé™ä½å¹¶å‘æ•°å‡å°‘APIå‹åŠ›ï¼‰
    process_json_file(input_file, output_file, concurrency=3)